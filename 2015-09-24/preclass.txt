0.  How much time did you spend on this pre-class exercise, and when?

	So far it has taken me an hour, the night before the class.
	And then 30 minutes after class.

1.  What are one or two points that you found least clear in the
    9/24 slide decks (including the narration)?


    It was a little unclear though whether the sequentially
    consistent shared model is used for estimating run time or
    simply mapping out the logic of a parallel program.


2.  The omp_mc.c file in the demo subdirectory runs a Monte Carlo
    simulation to estimate the expected value of a uniform random
    variable.  The "-p" option sets the number of processors used,
    while "-b" sets the number of trials between synchronizations.

    a) Write a model for the run time for this simulation code in
       terms of the number of trials (N), number of processors (p),
       time per trial (t_trial), and time to update the global
       counters in the critical section (t_update).


    The model used in the pre-class questions for 9-22-15 should
    work here as well.
    Let nbatch be the size of a batch of calculations being performe
    by a processor before synchronization. Then a model of the run time
    is the sum of the computation time (done in parallel) and the
    synchronization time (done in serial).

    t_elapsed = (N/p)*t_trial + t_update*(N/(p*nbatch))


    b) Run the code with a few different parameter values in order
       to estimate N, t_trial, and t_update for this code on
       a totient compute node.

    Some results:

    p=4 & b=1: time = 1.4e-04
    p=8 & b=1: time = 2.02e-04
    p=16 & b=1: time = 1.756e-04
    p=4 & b=4: time = 8.2127e-02
    p=8 & b=4: time = 8.358e-02
    p=16 & b=4: time = 8.3965e-02


    Results from class demonstration:
    t_update ~= 5.336e-08 
    t_trial ~= 6.966e-09 

    c) Based on your model, suggest a strategy for choosing the batch
       size.  How might you generalize this strategy to automatically
       choose batch sizes for different types of computational
       experiments?

    We would like to choose the largest batchsize until the costs of
    running extra (wasted) trials becomes too high. We must also be
    concerned about the processor time wasted by waiting for the slowest
    processor to finish its batch before synchronization.


3.  The "OpenMP pitfalls" paper describes some common pitfalls (both
    performance and correctness) in OpenMP codes.  Go through the
    checklist in the paper for omp_mc.c.  What performance mistakes
    are there in the demonstration implementation?

    In the critical section within thread_main( ), we can take the
    done_flag update operation and put it in a separate atomic section.
    This would cut down on the time all processors spend in the critical
    section.

    We may also want to do a flush() operation when we read a shared
    variable like nbatch, but it probably doesn't matter in this setting
    because nbatch remains constant.