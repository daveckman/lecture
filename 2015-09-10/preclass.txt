1. Look up the specs for the totient nodes. Having read the Roofline paper,
   draw a roofline diagram for one totient node (assuming only the
   host cores are used, for the moment).  How do things change with
   the addition of the two Phi boards?

The totient nodes are Intel Xeon E5-2620 v3 processors with 32 GB RAM per node.
From the preclass questions to 08-27, we calculated that the peak flop rate
for one of these nodes was 120 GFlops/sec (with Max Turbo). From the spec sheet found
online, the max memory bandwith is 59 GB/sec. From the roofline model, there is
a ridgepoint near an operational intensity of 2 flops/byte. This is shown in 
Roofline_figure.png.

With two Phi boards, we add 2,022 GFlops/sec (again from preclass questions from 08-27)
to reach a new peak flop rate of 2,142 GFlops/sec. The max memory bandwith of each 
of these boards is 320 GB/sec. Therefore 59 GB/sec is still the limiting factor.
The ridgepoint will now move over to an operational intensity of 2,142/59 = 36 flops/byte.

2. What is the difference between two cores and one core with
   hyperthreading?

A hyperthreaded core can act as two independent cores. The "processors" created
by the hyperthreaded core will have shared memory which the standard two cores
will have their own memories.

3. Do a Google search to find a picture of how memories are arranged
   on the Phi architecture.  Describe the setup briefly in your own
   words.  Is the memory access uniform or non-uniform?

I found pictures of the Phi memory architecture at:
https://software.intel.com/en-us/articles/intel-xeon-phi-coprocessor-codename-knights-corner

A Xeon Phi coprocessor as 60 nodes, each of which has a private L2 cache that is
communicated with a tagged directory (in main memory?). An interconnect allows for
communication between cores. The webpage says that the memory addresses are uniformly
distributed among the tag directories. From this description, I believe the memory
access is uniform.

4. Consider the parallel dot product implementations suggested in the
   slides.  As a function of the number of processors, the size of the
   vectors, and typical time to send a message, can you predict the
   speedup associated with parallelizing a dot product computation?
   [Note that dot products have low arithmetic intensity -- the
    roofline model may be useful for reasoning about the peak
    performance for computing pieces of the dot product]

Let N be the length of the vectors.
Suppose it takes A time units to perform an addition operation and M time units to
perform a multiplication operation (independent of the size of the floating
point numbers).

The serial time is then: A*N + M*N

If the number of processors is p and the typical time to send a message is t.

Assuming the addition is all done on one node at the end, the parallel time is
roughly: (N/p)*A + t + M*N.

The speedup would be:
S(p) = (A*N + M*N)/((N/p)*A + t + M*N)


